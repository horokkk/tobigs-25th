{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d41915c",
   "metadata": {},
   "source": [
    "# 투빅스 25기 이유민 1주차 2교시 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b98b0c",
   "metadata": {},
   "source": [
    "### Q1. 이론문제(서술형)\n",
    "#### 우리는 신경망이 단순히 선형변환(행렬 곱)만으로 이루어지지 않고, 중간에 비선형 함수(Non-linear function)가 반드시 필요하다는 것을 배웠습니다.\n",
    "\n",
    "##### 1-1. 선형변환을 여러 번 겹쳐서 수행했을 때 여전히 '선형적'인 성질이 유지된다는 것이 어떤 의미인지 설명하고,\n",
    "\n",
    "##### 1-2. 이를 통해 딥러닝 모델에서 활성화 함수(Activation Function)와 같은 비선형성이 왜 필수적인지 서술하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b3d12",
   "metadata": {},
   "source": [
    "---\n",
    "**정답 입력**\n",
    "\n",
    "1-1 : 선형변환은 보통 행렬 곱으로 표현되는데, 따라서 선형변환을 여러 번 연속으로 적용해도 결과적으로는 '하나의 선형변환'으로 합쳐진다. 즉, 선형층을 여러 개 쌓아도 결국 하나의 행렬로 압축되어 선형모델이 되는 것이다.\n",
    "\n",
    "1-2 : 중간에 비선형 함수가 없으면 신경망 전체가 단순 선형모델로 귀결된다. 그렇게 되면 해당 모델은 현실 데이터의 비선형 관계 및 패턴을 학습하지 못한다. 또한, 계층적 특징 추출이 불가능하다. 따라서 활성화 함수를 넣으면 신경망이 직선 모델뿐만 아니라 복잡한 비선형 관계 및 함수를 학습할 수 있게 되므로 필수적인 요소이다.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daedf55",
   "metadata": {},
   "source": [
    "### Q2. 개념 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc15efb5",
   "metadata": {},
   "source": [
    "#### 2-1. **[내적(Dot Product)과 Norm]**   \n",
    "##### 두 벡터의 유사도를 판단하거나 길이를 구할 때 사용되는 함수입니다. 빈칸을 채워 함수를 완성해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f34a19fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_similarity_components(v1, v2):\n",
    "    \"\"\"\n",
    "    v1, v2: numpy array (1D vectors)\n",
    "    반환값: 두 벡터의 내적 값, v1의 norm, v2의 norm\n",
    "    \"\"\"\n",
    "    # 1. 두 벡터의 내적(Dot Product)을 계산하세요.\n",
    "    dot_prod = np.dot(v1, v2) \n",
    "    \n",
    "    # 2. 각 벡터의 크기(Norm, L2 Norm)를 계산하세요.\n",
    "    norm_v1 = np.linalg.norm(v1)\n",
    "    norm_v2 = np.linalg.norm(v2)\n",
    "    \n",
    "    return dot_prod, norm_v1, norm_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac9c2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내적값 : 56.0, v1의 norm : 5.0, v2의 norm : 13.0\n"
     ]
    }
   ],
   "source": [
    "# 함수 확인\n",
    "v1 = np.array([3.0, 4.0])  # 예시 벡터 1\n",
    "v2 = np.array([12.0, 5.0])  # 예시 벡터 2\n",
    "out = calculate_similarity_components(v1, v2)\n",
    "\n",
    "print(f\"내적값 : {out[0]}, v1의 norm : {out[1]}, v2의 norm : {out[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf443749",
   "metadata": {},
   "source": [
    "#### 2-2. **[선형변환 (행렬 곱)]** \n",
    "##### 입력 벡터를 다른 차원의 공간으로 매핑하는 선형변환 과정입니다. 빈칸을 채워 함수를 완성해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a7e127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_transformation(x, W, b):\n",
    "    \"\"\"\n",
    "    x: 입력 벡터 (input features)\n",
    "    W: 가중치 행렬 (weight matrix)\n",
    "    b: 편향 벡터 (bias)\n",
    "    반환값: 선형변환 결과 z = Wx + b\n",
    "    \"\"\"\n",
    "    # 1. 행렬 곱(Matrix Multiplication) Wx를 수행하세요.\n",
    "    # 힌트: numpy의 행렬 곱 연산자(@) 또는 np.dot, np.matmul 사용\n",
    "    wx = W @ x\n",
    "    \n",
    "    # 2. 편향(bias)을 더하세요.\n",
    "    z = wx + b\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01b29ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6 0.3]\n"
     ]
    }
   ],
   "source": [
    "# 함수 확인\n",
    "x = np.array([1.0, 2.0])  # 예시 input vector\n",
    "W = np.array([[0.5, 1.0], [1.5, -0.5]])  # 예시 weight matrix\n",
    "b = np.array([0.1, -0.2])  # 예시 bias vector\n",
    "\n",
    "print(linear_transformation(x, W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1125f25d",
   "metadata": {},
   "source": [
    "#### 2-3. **[비선형성 (ReLU 함수)]** \n",
    "##### 선형변환 결과에 비선형성을 부여하는 활성화 함수입니다. 빈칸을 채워 함수를 완성해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e5215",
   "metadata": {},
   "source": [
    "![ReLU](https://velog.velcdn.com/images/sasganamabeer/post/4ea7fb74-2c3a-423f-8e47-bde64cddebb2/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%2C%202021-05-13%2015-27-54.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cc17868",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_function(z):\n",
    "    \"\"\"\n",
    "    z: 선형변환을 거친 값\n",
    "    반환값: 0보다 작으면 0, 0보다 크면 그대로 반환 (max(0, z))\n",
    "    \"\"\"\n",
    "    # numpy의 maximum 함수 등을 사용하여 ReLU를 구현하세요.\n",
    "    a = np.maximum(0, z)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c1447",
   "metadata": {},
   "source": [
    "### Q3. 응용 문제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1508a9",
   "metadata": {},
   "source": [
    "위 2번 문제에서 작성한 힘수들(calculate_similarity_components, linear_transformation, relu_function)을 활용하여 아래 문제를 해결하는 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1474f9b",
   "metadata": {},
   "source": [
    "#### 간단한 유사도 기반 분류기 구현.\n",
    "입력 데이터 x가 주어졌을 때, 1차적으로 선형 변환과 비선형 변환을 거쳐 특징(feature)을 추출하고, 이 추출된 특징이 기준 벡터(reference vector)와 얼마나 유사한지 코사인 유사도(Cosine Similarity)로 판단합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4d9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_evaluate(x, W, b, ref_vector):\n",
    "\n",
    "    # 1. 선형 변환\n",
    "    z = linear_transformation(x, W, b)\n",
    "\n",
    "    # 2. 비선형 활성화\n",
    "    feature = relu_function(z)\n",
    "\n",
    "    # 3. 코사인 유사도 계산\n",
    "    dot_prod, norm_f, norm_ref = calculate_similarity_components(feature, ref_vector)\n",
    "\n",
    "    # 코사인 유사도 공식 적용 (분모가 0이 아님을 가정)\n",
    "    cosine_sim = dot_prod / (norm_f * norm_ref)\n",
    "\n",
    "    return cosine_sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c04901de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6708203932499369\n"
     ]
    }
   ],
   "source": [
    "# 테스트 코드\n",
    "x = np.array([1, 2])\n",
    "W = np.array([[1, -1], [2, 0], [0, 1]]) # 3x2 행렬\n",
    "b = np.array([0, 1, -1])\n",
    "ref = np.array([1, 1, 0])\n",
    "print(forward_and_evaluate(x, W, b, ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d09e9a",
   "metadata": {},
   "source": [
    "+자유롭게 다른 활성화함수를 정의하는 등을 통해 추가적으로 학습하셔도 좋을것 같습니다.\n",
    "\n",
    "고생하셨습니다.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d2d1b1",
   "metadata": {},
   "source": [
    "### **과제 완료 후 잊지말고 꼭 깃허브에 제출해주시기바랍니다.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
